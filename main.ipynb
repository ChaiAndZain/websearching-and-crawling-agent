{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "google_search_project_api_key = os.getenv('google_search_api_key')\n",
    "google_search_project_id = os.getenv('google_search_project_id')\n",
    "google_gemini_api_key = os.getenv('google_gemini_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "        model='gemini-1.5-pro',\n",
    "        temperature=0.9,\n",
    "        google_api_key=google_gemini_api_key,\n",
    "    )\n",
    "\n",
    "\n",
    "import json\n",
    "def string_to_json(string):\n",
    "    try:\n",
    "        string = string.content.replace('```json', '').replace('```', '').strip()\n",
    "        json_obj = json.loads(string)\n",
    "        return json_obj\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON string\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# response = llm.invoke('how many calories are in 100 grams of wheat flour roti in asia?(Give me short and to the point answer only. do not add markdowns, etc)')\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def googlesearch_results(query:str, number_of_results:int=10):\n",
    "    query = \"how many calories are in 100 grams of wheat flour roti in asia?\"\n",
    "    google_search_url_template = f\"https://www.googleapis.com/customsearch/v1?key={google_search_project_api_key}&cx={google_search_project_id}&q={query}&num={number_of_results}&gl=pk&cr=countryPK&hl=en&lr=lang_en\"\n",
    "    response = requests.get(google_search_url_template)\n",
    "    results = []\n",
    "    for item in response.json()['items']:\n",
    "        results.append(item['link'])\n",
    "    return results\n",
    "\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "def duckduckgo_results(query:str, num_results:int=10):\n",
    "    results = DDGS().text(\n",
    "        query+\" filetype:html\",\n",
    "        max_results=num_results, region=\"pk\"\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in results:\n",
    "        result_list.append(result[\"href\"])\n",
    "    return result_list\n",
    "\n",
    "# from googlesearch import search\n",
    "# def googlesearch_results(query:str, num_results:int=10):\n",
    "#     result_obj = search(\n",
    "#         query, num_results=num_results,\n",
    "#         lang=\"en\"\n",
    "#     )\n",
    "#  \n",
    "#     result_list = []\n",
    "#     for i in result_obj:\n",
    "#         result_list.append(i)\n",
    "#     return result_list\n",
    "\n",
    "def query_list_to_urls_results(query_list:list, num_results_per_query:int=3, search_engine_func:callable=None):\n",
    "    \"\"\"\n",
    "    Get web results from a list of queries\n",
    "    INPUT:\n",
    "        query_list: list of queries\n",
    "        num_results_per_query: number of results per query\n",
    "        search_engine_func: search engine function\n",
    "    OUTPUT:\n",
    "        web_results: list of web results\n",
    "    \n",
    "    Example:\n",
    "        web_results_urls_list = query_list_to_web_results_as_urls(json_res['search_query_list'], 5, duckduckgo_results)\n",
    "    \"\"\"\n",
    "    \n",
    "    web_results = []\n",
    "    for query in query_list:\n",
    "        web_results.extend(search_engine_func(query, num_results_per_query))\n",
    "    return list(set(web_results))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_results_urls = duckduckgo_results(\"how many calories are in 100 grams of wheat flour roti in asia?\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def load_webpages(website_url:list, async_mode:bool=False, requests_per_second:int=2):\n",
    "    \"\"\"\n",
    "    Load webpages from a list of urls\n",
    "\n",
    "    INPUT:\n",
    "        website_url: list of urls\n",
    "    OUTPUT:\n",
    "        langchain document object\n",
    "    \"\"\"\n",
    "    loader = WebBaseLoader(website_url, continue_on_failure=True, verify_ssl=False)\n",
    "    if async_mode:\n",
    "        loader.requests_per_second = requests_per_second\n",
    "        return loader.aload()\n",
    "    else:\n",
    "        return loader.load()\n",
    "\n",
    "\n",
    "def text_cleaner(text:str):\n",
    "    \"\"\"\n",
    "    Clean text from html tags, extra spaces, newlines, etc\n",
    "    INPUT:\n",
    "        text: string\n",
    "    OUTPUT:\n",
    "        cleaned_text: string\n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    cleaned_text = re.sub(r\"\\xa0|\\r|\\t\", \" \", cleaned_text)\n",
    "    # cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\s{2,}\", \" \", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"<[^>]+>\", \"\", cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def langchain_document_cleaner(document_obj):\n",
    "    \"\"\"\n",
    "    Clean page_content of langchain document object\n",
    "    INPUT:\n",
    "        document: langchain document object\n",
    "    OUTPUT:\n",
    "        cleaned_document: langchain document object\n",
    "    \"\"\"\n",
    "    for i in range(len(document_obj)):\n",
    "        document_obj[i].page_content = text_cleaner(document_obj[i].page_content)\n",
    "    return document_obj\n",
    "\n",
    "def loader_with_cleaner(website_url:list, async_mode:bool=False, requests_per_second:int=2):\n",
    "    \"\"\"\n",
    "    Load webpages from a list of urls and clean them\n",
    "    INPUT:\n",
    "        website_url: list of urls\n",
    "    OUTPUT:\n",
    "        langchain document object\n",
    "    \"\"\"\n",
    "    return langchain_document_cleaner(load_webpages(website_url, async_mode, requests_per_second))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = loader_with_cleaner(web_results_urls, async_mode=True, requests_per_second=3)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# all-MiniLM-L12-v2 model has a limit of 256 words. ==> 4 x 256 = 1024 characters max\n",
    "def splitter(documents, chunk_size:int=900, chunk_overlap:int=200):\n",
    "    \"\"\"\n",
    "    Get chunks of text from the documents\n",
    "    INPUT:\n",
    "        documents: langchain document objects\n",
    "        chunk_size: size of each chunk\n",
    "        chunk_overlap: overlap between chunks\n",
    "    OUTPUT:\n",
    "        chunks: langchain document objects\n",
    "    \"\"\"\n",
    "    s = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)\n",
    "    chunks = s.split_documents(documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = splitter(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Working\\Calories Calculator\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.06030871719121933, 0.023757167160511017, 0.03897421434521675]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L12-v2\", cache_folder=\"temp\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", cache_folder=\"temp\")\n",
    "gc.collect()\n",
    "\n",
    "embeddings.embed_query(\"Warm Up\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "async def get_vDB(chunks, embeddings, folder_path:str=None, index_name:str=None, async_mode:bool=False):\n",
    "    '''\n",
    "    Get vector database from chunks\n",
    "    INPUT:\n",
    "        chunks: langchain document objects\n",
    "        embeddings: langchain embeddings object\n",
    "        folder_path: path to save the index\n",
    "        index_name: name of the index\n",
    "        async_mode: bool\n",
    "    OUTPUT:\n",
    "        vector database\n",
    "    '''\n",
    "    if async_mode:\n",
    "        db = await FAISS.afrom_documents(chunks, embeddings)\n",
    "    else:\n",
    "        db = FAISS.from_documents(chunks, embeddings)\n",
    "    if folder_path and index_name:\n",
    "        db.save_local(folder_path=folder_path, index_name=index_name)\n",
    "        print(f\"Index saved successfully at {folder_path}/{index_name}\")\n",
    "    return db\n",
    "\n",
    "\n",
    "def get_similar_docs(query_list:list[str], db, docs_per_query:int=5):\n",
    "    \"\"\"\n",
    "    Get similar documents from the query\n",
    "    INPUT:\n",
    "        query: string\n",
    "        db: vector database\n",
    "        k: number of similar documents\n",
    "    OUTPUT:\n",
    "        similar_docs: list of similar documents\n",
    "    \"\"\"\n",
    "    docs_list = []\n",
    "    for query in query_list:\n",
    "        docs_list.extend(db.similarity_search(query, docs_per_query))\n",
    "    return docs_list\n",
    "\n",
    "def documents_to_contextText(documents):\n",
    "    temp = []\n",
    "    context_text = \"\"\n",
    "    for doc in documents:\n",
    "        page_content = doc.page_content\n",
    "        if page_content in temp:\n",
    "            continue\n",
    "        context_text += f\"CONTENT:\\n'{page_content}'\\n\\n\"\n",
    "        temp.append(page_content)\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = await get_vDB(chunks, embeddings, folder_path=\"vdbs\", index_name=\"index_2\", async_mode=True)\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    '''You are an expert content writer and RAG specialist. Your task is to:\n",
    "1. Rephrase the user query into list of concise search string optimized for search engines. Use multiple search queries if needed. and return as a list of strings.\n",
    "2. \"Use general values in search queries which have higher chances of existance on search engines, not specific numbers which is difficult to find (Example: if user asks for finding price of 340 grams of potatos, then you can search for general/standard/easy to find value such as 100 grams or 1 kilogram). \n",
    "3. Then calculate the result for specific numbers from the general/standard/easy to find value from search results. (Like instead of searching for \"calories in 340 grams of wheat flour roti in asia\", search for \"calories in 100 grams of wheat flour roti in asia\" and then calculate the value for 500 grams using values fom 100 grams)\n",
    "4. Maximum number of search queries should be 4.\n",
    "5. IMPORTANT: Always give answer as a valid JSON object with keys: \"original_query\", \"search_query_list\". Make sure the JSON is valid.\n",
    "\n",
    "Example:\n",
    "User Query: \"I'm feeling tired all the time. What foods can help?\"\n",
    "Answer:\n",
    "{{\n",
    "    \"original_query\": \"how many calories are in 500 grams of wheat flour roti in asia?\",\n",
    "    \"search_query\": [\"calories in 100g wheat flour roti in aisa\",\n",
    "                      \"calories in 1kg wheat flour roti in asia\",\n",
    "                      \"calories in wheat flour roti in asia\"],\n",
    "}}\n",
    "\n",
    "####################################################\n",
    "User Query: \"{user_query}\"\n",
    "Answer:\n",
    "\n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template_2 = PromptTemplate.from_template(\n",
    "    '''\n",
    "## Task:\n",
    "Answer user queries in an informative way, leveraging the provided context for calculations and responses.\n",
    "\n",
    "## Instructions:\n",
    "Respond directly to the user query, avoiding additional explanations or markdown.\n",
    "Utilize the given context for calculations and formulating answers.\n",
    "Strictly adhere to the provided context, omitting external information.\n",
    "\n",
    "## Input:\n",
    "> Context:\n",
    "{context}\n",
    "\n",
    "> User Query:{user_query}\n",
    "\n",
    "> Answer:'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Work Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved successfully at vdbs/index_5\n",
      "Step 3: Vector database is prepared and used\n",
      "Step 4: Final response from LLM is generated\n",
      "\n",
      "\n",
      "Answer:\n",
      " Allah decrees in the Quran, specifically in Surah Nisa, verses 4:11-12 and 4:176, the shares of inheritance for each eligible recipient. \n"
     ]
    }
   ],
   "source": [
    "user_query = \"Explain how Allah tell us the method of distribution of inheritance? Also mention the verse number and the name of the Surah.\"\n",
    "prompt_1 = prompt_template.format(user_query=user_query)\n",
    "llm_res_1 = llm.invoke(prompt_1)\n",
    "llm_res_1_json = string_to_json(llm_res_1)\n",
    "print(\"Step 1: LLM queries generated\")\n",
    "\n",
    "web_urls = query_list_to_urls_results(llm_res_1_json['search_query_list'], 3, duckduckgo_results)\n",
    "chunks = splitter(loader_with_cleaner(web_urls, async_mode=True, requests_per_second=2))\n",
    "print(\"Step 2: Data Loaded and cleaned\")\n",
    "\n",
    "db = await get_vDB(chunks, embeddings, folder_path=\"vdbs\", index_name=\"index_5\", async_mode=True)\n",
    "similar_chunks = get_similar_docs(llm_res_1_json['search_query_list'], db, 3)\n",
    "print(\"Step 3: Vector database is prepared and used\")\n",
    "\n",
    "context = documents_to_contextText(similar_chunks)\n",
    "prompt_2 = prompt_template_2.format(context=context, user_query=user_query)\n",
    "llm_res_2 = llm.invoke(prompt_2)\n",
    "gc.collect()\n",
    "print(\"Step 4: Final response from LLM is generated\")\n",
    "\n",
    "print(f\"\\n\\nAnswer:\\n {llm_res_2.content.strip()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n    \"original_query\": \"Explain how Allah tell us the method of distribution of inheritance? Also mention the verse number and the name of the Surah.\",\\n    \"search_query_list\": [\\n        \"islamic inheritance laws verse\",\\n        \"quran verses on inheritance distribution\",\\n        \"surah about inheritance in islam\",\\n        \"method of inheritance distribution in islam\"\\n    ]\\n}\\n```', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-56a07ffe-d12e-4056-9977-522f9314b04d-0', usage_metadata={'input_tokens': 396, 'output_tokens': 91, 'total_tokens': 487})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_res_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 396, 'output_tokens': 91, 'total_tokens': 487}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(llm_res_1)[\"usage_metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usage_calculator(response_list:list, input_price, output_price, token_per_unit:int=1000, round_off:int=6):\n",
    "    \"\"\" \n",
    "    INPUT:\n",
    "        response_list: list of responses from LLMs\n",
    "        input_price: price of per unit [default is 1000] input tokens\n",
    "        output_price: price of per unit [default is 1000] output tokens\n",
    "    OUTPUT:\n",
    "        cost: cost of the responses\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    input_price_per_token = input_price/token_per_unit\n",
    "    output_price_per_token = output_price/token_per_unit\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    \n",
    "    for response in response_list:\n",
    "        input_tokens += dict(response)[\"usage_metadata\"]['input_tokens']\n",
    "        output_tokens += dict(response)[\"usage_metadata\"]['output_tokens']\n",
    "    \n",
    "    input_cost = round(input_tokens * input_price_per_token, round_off)\n",
    "    output_cost = round(output_tokens * output_price_per_token, round_off)\n",
    "    \n",
    "    return {\n",
    "        \"cost\": {\n",
    "            \"total_cost\": input_cost + output_cost,\n",
    "            \"input_cost\": input_cost,\n",
    "            \"output_cost\": output_cost\n",
    "        },\n",
    "        \"token\": {\n",
    "            \"total_tokens\": input_tokens + output_tokens,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost': {'total_cost': 0.0062125,\n",
       "  'input_cost': 0.0048475,\n",
       "  'output_cost': 0.001365},\n",
       " 'token': {'total_tokens': 1515, 'input_tokens': 1385, 'output_tokens': 130}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage = usage_calculator(\n",
    "    [llm_res_1, llm_res_2],\n",
    "    input_price=3.5,\n",
    "    output_price=10.5,\n",
    "    token_per_unit=1e6,\n",
    "    round_off=10)\n",
    "\n",
    "usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0062125"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage['cost']['total_cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
